---
title: "Likelihood_ReadData_JB"
author: "Jennifer Blanc"
date: "2023-01-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidyverse)
```

## Load summary statistics and filter

```{r}
# Read in summary statistics data
df <- fread("~/Downloads/50_raw.gwas.imputed_v3.both_sexes.tsv")

# Remove low confidence variants
df_filtered <- df %>% filter(low_confidence_variant == FALSE)

# Remove non-significant variants
df_sig <- df_filtered %>% filter(pval < 5e-8)

# Separated column 1 into chromosome, base pair, reference allele, and alt. allele
df_split <- df_sig %>% separate(variant, c("chr", "bp", "ref", "alt"), sep = ":")

# Filtered out low minor allele frequency variants
df_af_filter <- df_split %>% filter(minor_AF > 0.01) %>% filter(chr!= "X")
```

## Assign SNPs to LD Blocks

```{r}
 # Read in ld block information
ld_block <- fread("~/scratch/fourier_ls-all_parsed.bed")

assign_SNP_to_block <- function(CHR, BP, block = ld_block) {
  
  # Filter blocks based on snp
  block_chr <- block %>% filter(chr == CHR)
  first_start <- as.numeric(block_chr[1, "start"])
  block_bp <- block_chr %>% filter( (start < BP & stop >= BP) | BP == first_start) 
  
  # Assign
  block_num <- as.numeric(block_bp[,"block_number"])
  return(block_num)  
}

##### TO BE DELETED
df_af_filter <- df_af_filter[1:1000,]

 # Add block info - takes a while 
df_blocks <- df_af_filter %>% 
    mutate(block = apply(., MARGIN = 1, FUN = function(params)assign_SNP_to_block(as.numeric(params[1]), as.numeric(params[2]))))
```

## Pick minimum p-value and get freq information
```{r}
# Pick the minimum p-value per block
df_minP <- df_blocks %>%
  group_by(block) %>%
  slice_min(pval, with_ties = F)
  
# Find the alternate allele frequency
df_alt_freq <- df_minP %>%
  mutate(alt_AF = case_when(alt == minor_allele ~ minor_AF, alt != minor_allele ~ (1 - minor_AF)))
  
# Find the increasing allele frequency
df_inc_freq <- df_alt_freq %>%
  mutate(inc_AF = case_when((beta >= 0) ~ alt_AF, (beta < 0) ~ (1 - alt_AF)))

# Find sign of minor allele
df_minor_sign <- df_inc_freq %>% 
  mutate(minor_sign = case_when((alt == minor_allele) ~ beta/abs(beta), (alt != minor_allele) ~ -1 * beta/abs(beta)))
```

## Likelihood Functions

```{r}
### Calculate Non-centrality parameter 
calc_NCP <- function(p, Nd, Nc, B, prev) {
  
  # Nd = number of cases
  # Nc = number of controls 
  # B = absolute value of liability scale effect size
  # prev = prevalence 
  # p = risk increasing allele freq 
  
  # Compute threshold 
  t <- qnorm(1 - prev)
  
  # Compute mean liability 
  mu_1 <- B * (1 -p)
  mu_2 <- -B * p
  sigma_2 <- 1 - (B^2 * p *(1 -p))
  
  # Conditional Probabilities 
  p_case_a1 <- 1 - pnorm(t, mean = mu_1, sd = sqrt(sigma_2))
  p_case_a2 <- 1 - pnorm(t, mean = mu_2, sd = sqrt(sigma_2))
  p_control_a1 <- pnorm(t, mean = mu_1, sd = sqrt(sigma_2))
  p_control_a2 <- pnorm(t, mean = mu_2, sd = sqrt(sigma_2))
  
  # Compute allele frequency conditional on status
  p_d1 <- (p_case_a1 * p) / (1 - pnorm(t, mean = 0, sd = 1))
  p_c1 <- (p_control_a1 * p) / pnorm(t, mean = 0, sd = 1)
  
  # Observed counts
  q1_o <- 2 * Nd * p_d1
  q2_o <- 2 * Nc * p_c1
  q3_o <- 2 * Nd * (1 - p_d1)
  q4_o <- 2 * Nc * (1 - p_c1)
  
  # Expected counts
  q1_e <- ((2 * (Nd*p_d1 + Nc*p_c1)) * (2*Nd)) / (2* (Nd + Nc))
  q2_e <- ((2 * (Nd*p_d1 + Nc*p_c1)) * (2*Nc)) / (2* (Nd + Nc))
  q3_e <- ((2 * (Nd*(1-p_d1) + Nc*(1- p_c1))) * (2*Nd)) / (2* (Nd + Nc))
  q4_e <- ((2 * (Nd*(1-p_d1) + Nc*(1- p_c1))) * (2*Nc)) / (2* (Nd + Nc))
  
  # Compute NCP 
  NCP <- ((q1_o - q1_e)^2 / q1_e) + ((q2_o - q2_e)^2 / q2_e) + ((q3_o - q3_e)^2 / q3_e) + ((q4_o - q4_e)^2 / q4_e)
  
  # Return NCP
  return(NCP)
}

## Function to calculate the likelihood of observing a sign at a given site 
## Eq 23
prob_discovery <- function(x, sign, n_cases, n_controls, B, alpha, prev) {
  
  # x = minor allele frequency
  # sign = sign of liability scale effect size
  # B = absolute value of liability scale effect size
  # alpha =  type 1 error (5e-8 for GWAS)
  # n_cases = number of cases 
  # n_controls = number of controls 
  # prev = prevalence 
  
  # If flip to appropriate increasing allele frequency for given sign 
  if (sign == 1) {
    p <- x
  } else if (sign == -1)  {
    p <- 1 - x 
  } else {
    stop("Please enter a valid sign")
  }

  # Calculate ncp 
  ncp <- calc_NCP(p=p, Nd=n_cases, Nc=n_controls, B=B, prev=prev)
  
  # Calculate power 
  power <- 1- pchisq(qchisq(1- alpha, df =1),df=1, ncp = ncp)
  
  return(power)
  
}


# Function to calculate the probability that the minor allele is trait increasing, given its frequency and the selection coefficient
prob_sign_evo <- function(x, gamma) {
  
  # x = minor allele frequency
  # gamma = population scale selection coefficient
  
  result <- 1 / (1 + exp(gamma * (2*x - 1)))
  return(result)
}

# Function to compute join likelihood 
compute_joint_likelihood <- function(vec_x, obs_sign, B, n_cases, n_controls, alpha, prev, gamma) {
  
  # vec_x = vector of minor allele frequencies
  # obs_sign = observed sign
  
  # n_cases = number of cases 
  # n_controls = number of controls
  # B = absolute value of liability scale effect size
  # alpha =  type 1 error (5e-8 for GWAS)
  # prev = prevalence
  # gamma = population scaled selection coefficient
  
  L <- length(vec_x)
  vec_like <- rep(0, L)

  for(i in 1:L) {
    P1 <- prob_sign_evo(vec_x[i], gamma) * prob_discovery(x=vec_x[i], sign=1,n_cases=n_cases, n_controls = n_controls, B=B[i], alpha = alpha, prev = prev)
    P2  <- (1 - prob_sign_evo(vec_x[i], gamma)) * prob_discovery(x=vec_x[i], sign=-1,n_cases=n_cases, n_controls = n_controls, B=B[i], alpha = alpha, prev = prev)
   
    if (obs_sign[i] == 1) {
      vec_like[i] <- P1 / (P1 + P2)
    } else if (obs_sign[i] == -1) {
      vec_like[i] <- P2 / (P1 + P2)
    } else {
      stop("Enter valid signs")
    }
  }
  
  final_lik <- sum(log(vec_like))
  return(final_lik)
}
```

## Read in real data and calculate prevalence
```{r}
df <- fread("~/Desktop/df_minor_sign_both_eyes.csv")
phenos <- fread("~/Downloads/phenotypes.both_sexes.tsv")
phenos <- phenos %>% filter(phenotype == "5182_1")
prev <- phenos$n_cases / (phenos$n_controls + phenos$n_cases)
```

## Test likelihood function 
```{r}
compute_joint_likelihood(vec_x = df$minor_AF, obs_sign = df$minor_sign, B = abs(df$beta), n_cases = phenos$n_cases, n_controls = phenos$n_controls, alpha = 1e-5, prev = prev, gamma = 0)
```

##  Find the MLE 
```{r}
grid_search <- function(vec_x, obs_sign, n_cases, n_controls, B, alpha, prev, grid_vec) {

  # vec_x =  vector of minor allele frequency
  # obs_sign = vector of observed sign
  # n_cases = number of cases
  # n_controls = number of controls
  # B =  vector of absolute value of liability scale effect sizes
  # alpha =  type 1 error (5e-8 for GWAS)
  # prev = prevalence
  # grid_vec = vector of possible gamma values
  # Loop through possible gamma values, computing likelihood for each

  L <- length(vec_x)
  collect_liks <- rep(0, length(grid_vec))
  for (i in 1:length(grid_vec)) {
    collect_liks[i] <- compute_joint_likelihood(vec_x = vec_x, obs_sign = obs_sign , B = B, n_cases = n_cases, n_controls = n_controls, alpha = alpha, prev = prev, gamma = grid_vec[i])
  }
  # Return vector of likelihoods
  return(collect_liks)
}

grid_vec <- seq(0, 10, 0.1)
out <- grid_search(vec_x = df$minor_AF, obs_sign = df$minor_sign, B = abs(df$beta),n_cases = phenos$n_cases, n_controls = phenos$n_controls, alpha = 1e-5, prev = prev, grid_vec = grid_vec)

plot(grid_vec, out)
abline(v = grid_vec[which.max(out)], col = "red")
```

```{r}
x <- rbinom(50,2, 0.5)
pA <- sum(x[1:25]) / (50)
pB <- sum(x[26:50]) / (50)
xm <- x - mean(x)

c <- c(rep(1/2, 25), rep(-1/2, 25))
ep <- rnorm(50, 0, 0.1) 
E <- c + ep

mod <- lm(E ~ xm)
Bhat <- coef(mod)[2]
(t(xm) %*% E * (2/50)) * (1/(t(xm) %*% xm * (2/50)))

(pA - pB) / ((2/50) * t(xm) %*% xm) +(t(xm) %*% ep) * (1/(t(xm) %*% xm))

t(xm) %*% E * (1/25)
(pA - pB)
```

```{r}
A <- matrix(c(1.5, 0.5, 0.5, 1.5), nrow = 2)
D <- A
B <- matrix(c(0, 0, 0, 0), nrow = 2)
C <- B

P <- rbind(cbind(A, B), cbind(C,D))
Pinv <- solve(P)
Tvec <- c(rep(0.5, 2), rep(-0.5,2))


X <- matrix(rbinom(40, 2, 0.5),nrow =4)
pC <- colSums(X[1:2,])/4
pD <- colSums(X[3:4,])/4
X <- scale(X,scale = F)
q <- (t(Tvec) %*% Pinv %*% X) * as.numeric((1/(t(Tvec) %*% Pinv %*% Tvec)))
q
2* (pC - pD)
```

```{r}
(t(Tvec) %*% Pinv %*% X)
```

